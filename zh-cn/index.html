<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="robots" content="index,follow"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="HandheldFriendly" content="True"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>罗经周</title><meta name="keywords" content="具身智能, 多模态理解"><meta name="description" content="中山大学硕士研究生，专注于具身智能、多模态理解与计算机视觉领域。"><meta name="theme-color" content="#FFFFFF"><meta name="msapplication-TileColor" content="#1BC3FB"><meta name="msapplication-config" content="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/favicon/browserconfig.xml"><link rel="shortcut icon" type="image/x-icon" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="icon" type="image/x-icon" sizes="32x32" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="mask-icon" color="#1BC3FB" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="manifest" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/css/page.css"><meta name="generator" content="Hexo 6.3.0"></head><body class="docs"><div id="mobile-bar"><a class="menu-button fas fa-bars"></a><h2 class="nav-title">罗经周</h2></div><script src="https://cdn.jsdelivr.net/npm/vue@2.6/dist/vue.min.js"></script><script>Vue.config.productionTip=!1,window.PAGE_TYPE="resume"</script><div id="main" class="fix-sidebar"><div class="sidebar"><div class="sidebar-inner"><header class="header"><h2>罗经周</h2><div class="meta"><p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/LZ-CH"><i class="fab fa-github fa-fw"></i>&nbsp;Github 页面</a></p><p><a href="mailto:luojzh5@mail2.sysu.edu.cn" rel="external nofollow noopener noreferrer" target="_blank"><i class="fas fa-envelope fa-fw"></i>&nbsp;luojzh5@mail2.sysu.edu.cn</a></p><p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.sysu.edu.cn/"><i class="fas fa-map-marker-alt fa-fw"></i>&nbsp;中国广州</a></p></div><hr><br></header><div class="list"><ul class="menu-root"><li><a href="/index.html" class="sidebar-link current"></a></li><li><a href="/index_copy.html" class="sidebar-link current"></a></li><li><a href="/zh-cn/index.html" class="sidebar-link current"></a></li></ul></div></div></div><div class="content with-sidebar"><div class="resume-header"><div class="avatar"><img no-lazy class="avatar" src="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"></div><h1 class="resume-title">罗经周</h1><p class="description">中山大学硕士研究生，专注于具身智能、多模态理解与计算机视觉领域。</p></div><center><a href="/">English</a> | <a href="/zh-cn/">简体中文</a></center><h2 id="🔥-最新动态"><a href="#🔥-最新动态" class="headerlink" title="🔥 最新动态"></a>🔥 最新动态</h2><ul><li>2025.03: 📝 担任 IROS 2025 的审稿人。</li><li>2025.02: 🎉 一篇论文被 CVPR 2025 接收。</li><li>2024.09: 🥇 获得中山大学一等奖学金。</li><li>2023.06: 🎉 以推免研究生身份加入中山大学的<a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.sysu-hcp.net/">人机物智能融合（HCP-I2）实验室</a>。<details><summary>历史</summary><ul><li>2022.12: 🥇 获得中山大学专业素质奖学金。</li><li>2022.12: 🥇 获得中山大学一等奖学金。</li><li>2022.06: 🥈 团队在AAAI 挑战赛: “以数据为中心的鲁棒机器学习” 中获得第 2 名。</li><li>2022.02: 🔢 团队在CVPR • UG2+挑战赛：”低光视频动作识别” 中获得第 4 名。</li></ul></details></li></ul><h2 id="📝-代表性论文"><a href="#📝-代表性论文" class="headerlink" title="📝 代表性论文"></a>📝 代表性论文</h2><div style="display:flex;align-items:center;margin-bottom:40px"><img src="https://cdn.vectorstock.com/i/500p/92/18/loading-bar-doodle-hand-drawn-icon-outline-vector-54539218.jpg" data-original="https://raw.githubusercontent.com/LZ-CH/DSPNet/main/docs/DSPNet.png" width="30%" style="border-radius:10px;margin-right:20px"><div style="display:flex;flex-direction:column;align-items:flex-start;justify-content:center"><strong>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</strong><p><strong>Jingzhou Luo</strong>, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin</p><strong>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025.</strong><div style="margin-top:8px">📰 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/abs/2503.03190">论文</a> 🔗 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.googleusercontent.com/scholar.bib?q=info:DzvZINTp3YIJ:scholar.google.com/&output=citation">BibTeX</a> <i class="fab fa-github"></i> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/LZ-CH/DSPNet">GitHub</a></div></div></div><hr><div style="display:flex;align-items:center;margin-bottom:40px"><img src="https://cdn.vectorstock.com/i/500p/92/18/loading-bar-doodle-hand-drawn-icon-outline-vector-54539218.jpg" data-original="https://ieeexplore.ieee.org/ielx7/7361/10599846/10184205/graphical_abstract/jsen-gagraphic-3294360.jpg" width="30%" style="border-radius:10px;margin-right:20px"><div style="display:flex;flex-direction:column;align-items:flex-start;justify-content:center"><strong>V-DixMatch: A Semi-Supervised Learning Method for Human Action Recognition in Night Video Sensing</strong><p>Chenxi Wang, <strong>Jingzhou Luo</strong>, Xing Luo, Haoran Qi, Zhi Jin</p><strong>IEEE Sensors Journal, 2023.</strong><div style="margin-top:8px">📰 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/10184205">论文</a> 🔗 <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.googleusercontent.com/scholar.bib?q=info:sbIMya7d7HYJ:scholar.google.com/&output=citation">BibTeX</a></div></div></div><hr><h2 id="📌-重点项目"><a href="#📌-重点项目" class="headerlink" title="📌 重点项目"></a>📌 重点项目</h2><div style="display:flex;align-items:center;margin-bottom:40px"><img src="https://cdn.vectorstock.com/i/500p/92/18/loading-bar-doodle-hand-drawn-icon-outline-vector-54539218.jpg" data-original="https://p0.itc.cn/q_70/images03/20230602/b3a3560684b847c1a99af1c98f42ea48.png" width="30%" style="border-radius:10px;margin-right:20px"><div style="display:flex;flex-direction:column;align-items:flex-start;justify-content:center"><strong>医学影像诊断报告生成</strong><br><strong>全球人工智能创新大赛，2023.</strong><div style="margin-top:8px"><i class="fab fa-github"></i> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/LZ-CH/GAIIC2023">GitHub</a></div></div></div><hr><div style="display:flex;align-items:center;margin-bottom:40px"><img src="https://cdn.vectorstock.com/i/500p/92/18/loading-bar-doodle-hand-drawn-icon-outline-vector-54539218.jpg" data-original="https://user-images.githubusercontent.com/37669469/126025119-ce29fe07-bcf0-4384-8a16-424adff4933d.jpg" width="30%" style="border-radius:10px;margin-right:20px"><div style="display:flex;flex-direction:column;align-items:flex-start;justify-content:center"><strong>Learning Multi-Scale Photo Exposure Correction</strong><br><strong>一篇CVPR的Pytorch复现, 2022.</strong><div style="margin-top:8px"><i class="fab fa-github"></i> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/LZ-CH/Exposure_Correction-pytorch">GitHub</a></div></div></div><h2 id="🏆-奖项与竞赛"><a href="#🏆-奖项与竞赛" class="headerlink" title="🏆 奖项与竞赛"></a>🏆 奖项与竞赛</h2><p><strong>🎓 荣誉</strong></p><table><thead><tr><th>📜 奖项</th><th>🏫 机构</th><th>📅 年份</th></tr></thead><tbody><tr><td>🥇 <strong>一等奖学金</strong></td><td>中山大学</td><td>2024</td></tr><tr><td>🏅 <strong>专业素质奖学金</strong></td><td>中山大学</td><td>2022</td></tr></tbody></table><p><strong>🏅 竞赛</strong></p><table><thead><tr><th>🏆 名次</th><th>🏁 竞赛名称</th><th>📅 年份</th></tr></thead><tbody><tr><td>🥉 <strong>三等奖</strong></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.dataarobotics.com/zh/blog/news-392.html">中国软件大会 • 达闼杯：机器人大模型与身身智能挑战赛</a></td><td>2023</td></tr><tr><td>🏁 <strong>第 12 名</strong> <em>(12&#x2F;1100)</em></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.heywhale.com/org/gaiic2023/competition/area/63fef766b4422ee27402289d/content">全球人工智能技术创新大赛: 医学影像诊断报告生成</a></td><td>2023</td></tr><tr><td>🥈 <strong>第 2 名</strong> <em>(2&#x2F;3691)</em></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://tianchi.aliyun.com/competition/entrance/531939/introduction">AAAI: 基于数据的机器学习模型鲁棒性</a></td><td>2022</td></tr><tr><td>🔢 <strong>第 4 名</strong></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://cvpr2022.ug2challenge.org/program21/track2.html">CVPR • UG2+ 基于半监督学习的低光照视频行为识别</a></td><td>2022</td></tr><tr><td>🏁 <strong>第 8 名</strong> <em>(8&#x2F;10006)</em></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://tianchi.aliyun.com/competition/entrance/531929/introduction">GSICS: 智能预测糖尿病性黄斑水肿(DME)患者的Anti-VEGF治疗转归</a></td><td>2021</td></tr></tbody></table><h2 id="🎓-教育经历"><a href="#🎓-教育经历" class="headerlink" title="🎓 教育经历"></a>🎓 教育经历</h2><ul><li>📖 <strong>2023.09 - 2026.06:</strong> 计算机技术，硕士，中山大学 🏫<ul><li>🔬 <strong>研究方向:</strong> 具身智能三维场景感知</li><li>🧑‍🏫 <strong>导师:</strong> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com.hk/citations?user=Nav8m8gAAAAJ&hl=zh-CN">林倞</a>教授, <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com/citations?user=l0z2QNQAAAAJ&hl=zh-CN">刘阳</a>教授</li></ul></li><li>📖 <strong>2019.09 - 2023.07:</strong> 智能科学与技术，本科，中山大学 🏫<ul><li>🔬 <strong>研究方向:</strong> 低光照图像增强</li><li>🧑‍🏫 <strong>导师:</strong> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com.hk/citations?user=v70dNBoAAAAJ&hl=zh-CN">金枝</a>教授</li></ul></li></ul><h2 id="💻-实习经历"><a href="#💻-实习经历" class="headerlink" title="💻 实习经历"></a>💻 实习经历</h2><ul><li>2024.12 - 至今: 华为诺亚方舟实验室，导师：池达丰，<a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com/citations?hl=zh-CN&user=ny9KAREAAAAJ">庄雨铮</a>，<a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com/citations?hl=zh-CN&user=kxs0KVIAAAAJ">黄国位</a></li></ul></div></div><script src="https://cdn.jsdelivr.net/gh/xaoxuu/hexo-theme-resume@1.0.0/source/js/smooth-scroll.min.js"></script><footer id="footer"><p></p><p><a href="/">Copyright © 2023-2024</a></p><p></p></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.5/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/xaoxuu/hexo-theme-resume@1.0.0/source/js/common.js"></script><script src="https://cdn.jsdelivr.net/gh/xaoxuu/hexo-theme-resume@1.0.0/source/js/css.escape.js"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5/dist/jquery.fancybox.min.js"></script><script>let LAZY_LOAD_IMAGE = "[object Object]";
        $("body").find("fancybox").find("img").each(function () {
            var element = document.createElement("a");
            $(element).attr("data-fancybox", "gallery");
            $(element).attr("href", $(this).attr("src"));
             if (LAZY_LOAD_IMAGE) {
               $(element).attr("href", $(this).attr("data-original"));
             }
            $(this).wrap(element);
        });</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,d=o();function o(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=o());for(var e,i=0;i<d.length;i++)0<=(e=(e=d[i]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,a,n,o=d[i];e=function(){d=d.filter(function(t){return o!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(o)},(t=o).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,n=t.getAttribute("data-original"),a.onload=function(){t.src=n,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=n},t.src!==n&&(a.src=n)))}()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)}(this);</script></body></html>