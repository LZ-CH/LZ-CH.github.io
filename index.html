<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="robots" content="index,follow"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="HandheldFriendly" content="True"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Jingzhou Luo</title><meta name="keywords" content="Resume, Embodied AI, Multi-modality Understanding, NLP"><meta name="description" content="M.S. at Sun Yat-sen University, focusing on Embodied AI, Multi-modality Understanding and Computer Vision."><meta name="theme-color" content="#FFFFFF"><meta name="msapplication-TileColor" content="#1BC3FB"><meta name="msapplication-config" content="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-assets/favicon/browserconfig.xml"><link rel="shortcut icon" type="image/x-icon" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="icon" type="image/x-icon" sizes="32x32" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="mask-icon" color="#1BC3FB" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="manifest" href="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/css/page.css"><meta name="generator" content="Hexo 6.3.0"></head><body class="docs"><div id="mobile-bar"><a class="menu-button fas fa-bars"></a><h2 class="nav-title">Jingzhou Luo</h2></div><script src="https://cdn.jsdelivr.net/npm/vue@2.6/dist/vue.min.js"></script><script>Vue.config.productionTip=!1,window.PAGE_TYPE="resume"</script><div id="main" class="fix-sidebar"><div class="sidebar"><div class="sidebar-inner"><header class="header"><h2>Jingzhou Luo</h2><div class="meta"><p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/LZ-CH"><i class="fab fa-github fa-fw"></i>&nbsp;Github page</a></p><p><a href="mailto:luojzh5@mail2.sysu.edu.cn" rel="external nofollow noopener noreferrer" target="_blank"><i class="fas fa-envelope fa-fw"></i>&nbsp;luojzh5@mail2.sysu.edu.cn</a></p><p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.sysu.edu.cn/"><i class="fas fa-map-marker-alt fa-fw"></i>&nbsp;Guangzhou, China</a></p></div><hr><br></header><div class="list"><ul class="menu-root"><li><a href="/index.html" class="sidebar-link current"></a></li><li><a href="/zh-cn/index.html" class="sidebar-link current"></a></li><li><a href="/index_copy.html" class="sidebar-link current"></a></li></ul></div></div></div><div class="content with-sidebar"><div class="resume-header"><div class="avatar"><img no-lazy class="avatar" src="https://avatars.githubusercontent.com/u/61136294?s=400&u=25cf4dee81585071b5bfe2df22315cd0bb2f83e8&v=4"></div><h1 class="resume-title">Jingzhou Luo</h1><p class="description">M.S. at Sun Yat-sen University, focusing on Embodied AI, Multi-modality Understanding and Computer Vision.</p></div><center><a href="/">English</a> | <a href="/zh-cn/">ç®€ä½“ä¸­æ–‡</a></center><h2 id="ğŸ”¥-News"><a href="#ğŸ”¥-News" class="headerlink" title="ğŸ”¥ News"></a>ğŸ”¥ News</h2><ul><li>2025.02: ğŸ‰ One paper is accepted at CVPR 2025.</li><li>2024.09: ğŸ¥‡ I receive the First-Class Scholarship from Sun Yat-sen University.</li><li>2023.06: ğŸ‰ I am admitted to the <a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.sysu-hcp.net/">Human-Cyber-Physical Intelligence Integration (HCP-l2) Lab</a> of Sun Yat-sen University as a recommended graduate student.<details><summary>Previous</summary><ul><li>2022.12: ğŸ¥‡ I receive Professional Quality Scholarship from Sun Yat-sen University.</li><li>2022.12: ğŸ¥‡ I receive the First-Class Scholarship from Sun Yat-sen University.</li><li>2022.06: ğŸ¥ˆ Our team ranks 2â¿áµˆ in AAAI Challenge: Data-centric Robust Learning on Machine Learning Models.</li><li>2022.02: ğŸ”¢ Our team ranks 4áµ—Ê° in CVPR â€¢ UG2+ Challenge: Semi-supervised Action Recognition in the Dark.</li></ul></details></li></ul><h2 id="ğŸ“-Selected-Publications"><a href="#ğŸ“-Selected-Publications" class="headerlink" title="ğŸ“ Selected Publications"></a>ğŸ“ Selected Publications</h2><div style="display:flex;align-items:center;margin-bottom:40px"><img src="https://cdn.vectorstock.com/i/500p/92/18/loading-bar-doodle-hand-drawn-icon-outline-vector-54539218.jpg" data-original="https://raw.githubusercontent.com/LZ-CH/DSPNet/main/docs/DSPNet.png" width="30%" style="border-radius:10px;margin-right:20px"><div style="display:flex;flex-direction:column;align-items:flex-start;justify-content:center"><strong>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</strong><p><strong>Jingzhou Luo</strong>, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin</p><strong>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025.</strong><div style="margin-top:8px">ğŸ“° <a target="_blank" rel="external nofollow noopener noreferrer" href="https://arxiv.org/abs/2503.03190">Paper</a> ğŸ”— <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.googleusercontent.com/scholar.bib?q=info:DzvZINTp3YIJ:scholar.google.com/&output=citation&scisdr=ClHX_Hd-EMDW63Mxnrg:AFWwaeYAAAAAZ803hriU4eSqR_ER7Xues4XW1pc&scisig=AFWwaeYAAAAAZ803hvP9JLc364eWs0dNevAseN4&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a> <i class="fab fa-github"></i> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/LZ-CH/DSPNet">GitHub</a></div></div></div><hr><div style="display:flex;align-items:center;margin-bottom:40px"><img src="https://cdn.vectorstock.com/i/500p/92/18/loading-bar-doodle-hand-drawn-icon-outline-vector-54539218.jpg" data-original="https://ieeexplore.ieee.org/ielx7/7361/10599846/10184205/graphical_abstract/jsen-gagraphic-3294360.jpg" width="30%" style="border-radius:10px;margin-right:20px"><div style="display:flex;flex-direction:column;align-items:flex-start;justify-content:center"><strong>V-DixMatch: A Semi-Supervised Learning Method for Human Action Recognition in Night Video Sensing</strong><p>Chenxi Wang, <strong>Jingzhou Luo</strong>, Xing Luo, Haoran Qi, Zhi Jin</p><strong>IEEE Sensors Journal, 2023.</strong><div style="margin-top:8px">ğŸ“° <a target="_blank" rel="external nofollow noopener noreferrer" href="https://ieeexplore.ieee.org/abstract/document/10184205">Paper</a> ğŸ”— <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.googleusercontent.com/scholar.bib?q=info:sbIMya7d7HYJ:scholar.google.com/&output=citation&scisdr=ClHX_Hd-EMDW63NBUtI:AFWwaeYAAAAAZ81HStLMTaK0lxwg5fM4W48LZIQ&scisig=AFWwaeYAAAAAZ81HSueZJyHxs8JLOogH0q_puRQ&scisf=4&ct=citation&cd=-1&hl=zh-CN">BibTeX</a></div></div></div><hr><h2 id="ğŸ“Œ-Featured-Projects"><a href="#ğŸ“Œ-Featured-Projects" class="headerlink" title="ğŸ“Œ Featured Projects"></a>ğŸ“Œ Featured Projects</h2><div style="display:flex;align-items:center;margin-bottom:40px"><img src="https://cdn.vectorstock.com/i/500p/92/18/loading-bar-doodle-hand-drawn-icon-outline-vector-54539218.jpg" data-original="https://p0.itc.cn/q_70/images03/20230602/b3a3560684b847c1a99af1c98f42ea48.png" width="30%" style="border-radius:10px;margin-right:20px"><div style="display:flex;flex-direction:column;align-items:flex-start;justify-content:center"><strong>Medical imaging diagnostic report generation</strong><br><strong>Global AI Innovation Competition, 2023.</strong><div style="margin-top:8px"><i class="fab fa-github"></i> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/LZ-CH/GAIIC2023">GitHub</a></div></div></div><hr><div style="display:flex;align-items:center;margin-bottom:40px"><img src="https://cdn.vectorstock.com/i/500p/92/18/loading-bar-doodle-hand-drawn-icon-outline-vector-54539218.jpg" data-original="https://user-images.githubusercontent.com/37669469/126025119-ce29fe07-bcf0-4384-8a16-424adff4933d.jpg" width="30%" style="border-radius:10px;margin-right:20px"><div style="display:flex;flex-direction:column;align-items:flex-start;justify-content:center"><strong>Learning Multi-Scale Photo Exposure Correction</strong><br><strong>Pytorch Reimplementation of CVPR, 2022.</strong><div style="margin-top:8px"><i class="fab fa-github"></i> <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/LZ-CH/Exposure_Correction-pytorch">GitHub</a></div></div></div><h2 id="ğŸ†-Awards"><a href="#ğŸ†-Awards" class="headerlink" title="ğŸ† Awards"></a>ğŸ† Awards</h2><p><strong>ğŸ“ Honors</strong></p><table><thead><tr><th>ğŸ“œ Award</th><th>ğŸ« Institution</th><th>ğŸ“… Year</th></tr></thead><tbody><tr><td>ğŸ¥‡ <strong>The First Prize Scholarship</strong></td><td>Sun Yat-sen University</td><td>2024</td></tr><tr><td>ğŸ… <strong>Professional Quality Scholarship</strong></td><td>Sun Yat-sen University</td><td>2022</td></tr></tbody></table><p><strong>ğŸ… Competitions</strong></p><table><thead><tr><th>ğŸ† Rank</th><th>ğŸ Competition</th><th>ğŸ“… Year</th></tr></thead><tbody><tr><td>ğŸ¥‰ <strong>3rd Prize</strong></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.dataarobotics.com/zh/blog/news-392.html">China Software Conference â€¢ DATA Cup: Large Model &amp; Embodied Intelligence</a></td><td>2023</td></tr><tr><td>ğŸ <strong>12th Place</strong> <em>(12&#x2F;1100)</em></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.heywhale.com/org/gaiic2023/competition/area/63fef766b4422ee27402289d/content">GAIIC: Medical Imaging Diagnostic Report Generation</a></td><td>2023</td></tr><tr><td>ğŸ¥ˆ <strong>2nd Place</strong> <em>(2&#x2F;3691)</em></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://tianchi.aliyun.com/competition/entrance/531939/introduction">AAAI: Data-centric Robust Learning on Machine Learning Models</a></td><td>2022</td></tr><tr><td>ğŸ”¢ <strong>4th Place</strong></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://cvpr2022.ug2challenge.org/program21/track2.html">CVPR â€¢ UG2+ Challenge: Low-Light Video Action Recognition</a></td><td>2022</td></tr><tr><td>ğŸ <strong>8th Place</strong> <em>(8&#x2F;10006)</em></td><td><a target="_blank" rel="external nofollow noopener noreferrer" href="https://tianchi.aliyun.com/competition/entrance/531929/introduction">GSICS: Prediction on DME Patientsâ€™ Response to Anti-VEGF Treatment</a></td><td>2021</td></tr></tbody></table><h2 id="ğŸ“-Education"><a href="#ğŸ“-Education" class="headerlink" title="ğŸ“ Education"></a>ğŸ“ Education</h2><ul><li>ğŸ“– <strong>2023.09 - 2026.06:</strong> M.S. in Computer Technology, Sun Yat-sen University ğŸ«<ul><li>ğŸ”¬ <strong>Research Area:</strong> Embodied 3D Scene Perception</li><li>ğŸ§‘â€ğŸ« <strong>Advisor:</strong> Prof. <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com.hk/citations?user=Nav8m8gAAAAJ&hl=zh-CN">Liang Lin</a>, Prof. <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com/citations?user=l0z2QNQAAAAJ&hl=zh-CN">Yang liu</a></li></ul></li><li>ğŸ“– <strong>2019.09 - 2023.07:</strong> B.S. in Intelligent Science and Technology, Sun Yat-sen University ğŸ«<ul><li>ğŸ”¬ <strong>Research Area:</strong> Low-light Image Enhancement</li><li>ğŸ§‘â€ğŸ« <strong>Advisor:</strong> Prof. <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com.hk/citations?user=v70dNBoAAAAJ&hl=zh-CN">Zhi Jin</a></li></ul></li></ul><h2 id="ğŸ’»-Internships"><a href="#ğŸ’»-Internships" class="headerlink" title="ğŸ’» Internships"></a>ğŸ’» Internships</h2><ul><li>2024.12 - present: Huawei Noahâ€™s Ark Lab, supervised by Dafeng Chi, <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com/citations?hl=zh-CN&user=ny9KAREAAAAJ">Yuzheng Zhuang</a> and <a target="_blank" rel="external nofollow noopener noreferrer" href="https://scholar.google.com/citations?hl=zh-CN&user=kxs0KVIAAAAJ">Guowei Huang</a></li></ul></div></div><script src="https://cdn.jsdelivr.net/gh/xaoxuu/hexo-theme-resume@1.0.0/source/js/smooth-scroll.min.js"></script><footer id="footer"><p></p><p><a href="/">Copyright Â© 2023-2024</a></p><p></p></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.5/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/xaoxuu/hexo-theme-resume@1.0.0/source/js/common.js"></script><script src="https://cdn.jsdelivr.net/gh/xaoxuu/hexo-theme-resume@1.0.0/source/js/css.escape.js"></script><script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5/dist/jquery.fancybox.min.js"></script><script>let LAZY_LOAD_IMAGE = "[object Object]";
        $("body").find("fancybox").find("img").each(function () {
            var element = document.createElement("a");
            $(element).attr("data-fancybox", "gallery");
            $(element).attr("href", $(this).attr("src"));
             if (LAZY_LOAD_IMAGE) {
               $(element).attr("href", $(this).attr("data-original"));
             }
            $(this).wrap(element);
        });</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:1,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})})</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,d=o();function o(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=o());for(var e,i=0;i<d.length;i++)0<=(e=(e=d[i]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,a,n,o=d[i];e=function(){d=d.filter(function(t){return o!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(o)},(t=o).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e()):(a=new Image,n=t.getAttribute("data-original"),a.onload=function(){t.src=n,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=n},t.src!==n&&(a.src=n)))}()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)}(this)</script><script>"use strict";if("serviceWorker"in navigator){navigator.serviceWorker.register("service-worker.js").then((function(reg){reg.onupdatefound=function(){var installingWorker=reg.installing;installingWorker.onstatechange=function(){switch(installingWorker.state){case"installed":if(navigator.serviceWorker.controller){console.log("New or updated content is available.")}else{console.log("Content is now available offline!")}break;case"redundant":console.error("The installing service worker became redundant.");break}}}}))["catch"]((function(e){console.error("Error during service worker registration:",e)}))}</script></body></html>